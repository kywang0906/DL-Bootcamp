{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOQPEyAjdsA/gU6L1UGh42s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","FOLDERNAME = \"Colab\\ Notebooks/WeHelp/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUB7GicIx8Aa","executionInfo":{"status":"ok","timestamp":1748198938887,"user_tz":-480,"elapsed":4651,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}},"outputId":"cb7fcf12-d22f-4c6a-e588-8f911c1c6eff"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqaEAx7ux_xo","executionInfo":{"status":"ok","timestamp":1748198938894,"user_tz":-480,"elapsed":5,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}},"outputId":"74760496-8b03-4b29-9e8f-3655d40a5dee"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/WeHelp\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"CNUMZlVGsIcj","executionInfo":{"status":"ok","timestamp":1748198940469,"user_tz":-480,"elapsed":1574,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}}},"outputs":[],"source":["import os\n","import math\n","import json\n","import sentencepiece as spm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Hyperparameters\n","BATCH_SIZE = 32\n","SEQ_LEN = 100\n","EMBED_DIM = 256\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","NUM_ENCODER_LAYERS = 6\n","NUM_DECODER_LAYERS = 6\n","DROPOUT = 0.1\n","NUM_EPOCHS = 30\n","LR = 1e-4\n","VOCAB_SIZE = 4000\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","JSON_PATH = 'data/data.json'\n","CORPUS_PATH = 'corpus.txt'\n","SP_MODEL_PREFIX = 'spm'\n","SP_MODEL_FILE = SP_MODEL_PREFIX + '.model'"]},{"cell_type":"markdown","source":["## Train SentencePiece"],"metadata":{"id":"bABqzazLGGnO"}},{"cell_type":"code","source":["# Extract raw paragraphs\n","with open(JSON_PATH, 'r', encoding='utf-8') as f:\n","    data = json.load(f)\n","paras = []\n","for entry in data:\n","    paras.extend(entry.get('paragraphs', []))\n","# Write to corpus.txt\n","with open(CORPUS_PATH, 'w', encoding='utf-8') as f:\n","    f.write('\\n'.join(paras))\n","# Train SP model on corpus\n","spm.SentencePieceTrainer.Train(\n","    f\"--input={CORPUS_PATH} --model_prefix={SP_MODEL_PREFIX} \"\n","    f\"--vocab_size={VOCAB_SIZE} --character_coverage=0.9995 \"\n","    f\"--model_type=bpe\"\n",")\n","# Load SP model\n","sp = spm.SentencePieceProcessor()\n","sp.Load(SP_MODEL_FILE)\n","sp = spm.SentencePieceProcessor()\n","sp.Load(SP_MODEL_FILE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFwUkjRXnozH","executionInfo":{"status":"ok","timestamp":1748198940786,"user_tz":-480,"elapsed":314,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}},"outputId":"0b4d7a5a-a1d2-44cb-8370-628933a0bbbd"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"qbLIEUJiGRW8"}},{"cell_type":"code","source":["class SubwordDataset(Dataset):\n","    def __init__(self, texts, sp, seq_len):\n","        # concatenate all paragraphs into one id list\n","        self.ids = []\n","        for t in texts:\n","            ids = [sp.bos_id()] + sp.EncodeAsIds(t) + [sp.eos_id()]\n","            self.ids.extend(ids)\n","        self.seq_len = seq_len\n","\n","    def __len__(self):\n","        return max(0, len(self.ids) - self.seq_len)\n","\n","    def __getitem__(self, idx):\n","        chunk = self.ids[idx : idx + self.seq_len + 1]\n","        src = torch.tensor(chunk[:-1], dtype=torch.long)\n","        tgt = torch.tensor(chunk[1:], dtype=torch.long)\n","        return src, tgt\n","\n","# collate function for sliding window\n","def collate_fn(batch):\n","    srcs = [x for x, _ in batch]\n","    tgts = [y for _, y in batch]\n","    src = torch.stack(srcs).transpose(0, 1).to(DEVICE)  # (seq_len, batch)\n","    tgt = torch.stack(tgts).transpose(0, 1).to(DEVICE)\n","    return src, tgt\n","\n","# prepare DataLoader\n","dataset = SubwordDataset(paras, sp, SEQ_LEN)\n","loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"],"metadata":{"id":"LeWhUGexyh4o","executionInfo":{"status":"ok","timestamp":1748198940843,"user_tz":-480,"elapsed":34,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"4CviNDMTGXGJ"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer('pe', pe.unsqueeze(1))\n","\n","    def forward(self, x):\n","        x = x + self.pe[: x.size(0)]\n","        return self.dropout(x)"],"metadata":{"id":"c-MWAfVSafQZ","executionInfo":{"status":"ok","timestamp":1748198940852,"user_tz":-480,"elapsed":7,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, EMBED_DIM)\n","        self.pos_enc = PositionalEncoding(EMBED_DIM, DROPOUT)\n","        self.transformer = nn.Transformer(\n","            d_model=EMBED_DIM,\n","            nhead=NHEAD,\n","            num_encoder_layers=NUM_ENCODER_LAYERS,\n","            num_decoder_layers=NUM_DECODER_LAYERS,\n","            dim_feedforward=FFN_HID_DIM,\n","            dropout=DROPOUT,\n","        )\n","        self.fc = nn.Linear(EMBED_DIM, vocab_size)\n","\n","    def forward(self, src, tgt, tgt_mask=None):\n","        src_emb = self.embed(src) * math.sqrt(EMBED_DIM)\n","        src_emb = self.pos_enc(src_emb)\n","        tgt_emb = self.embed(tgt) * math.sqrt(EMBED_DIM)\n","        tgt_emb = self.pos_enc(tgt_emb)\n","        out = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n","        return self.fc(out)"],"metadata":{"id":"ZiHrOu9NwMsw","executionInfo":{"status":"ok","timestamp":1748198940855,"user_tz":-480,"elapsed":1,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, loader, criterion, optimizer):\n","    model.train()\n","    total_loss = 0.0\n","    for src, tgt in loader:\n","        tgt_input = tgt[:-1, :]\n","        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(DEVICE)\n","        logits = model(src, tgt_input, tgt_mask)\n","        loss = criterion(logits.view(-1, logits.size(-1)), tgt[1:].reshape(-1))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)"],"metadata":{"id":"JELc4nVJwNa2","executionInfo":{"status":"ok","timestamp":1748198940861,"user_tz":-480,"elapsed":6,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Sampling Utilities"],"metadata":{"id":"ZCoz5KckGdmi"}},{"cell_type":"code","source":["def top_k_top_p_filtering(logits, top_k=50, top_p=0.9):\n","    # logits: 1D tensor\n","    if top_k > 0:\n","        values, _ = torch.topk(logits, top_k)\n","        min_val = values[-1]\n","        logits = torch.where(logits < min_val, torch.full_like(logits, -1e9), logits)\n","    if top_p < 1.0:\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n","        sorted_indices_to_remove[0] = False\n","        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","        logits[indices_to_remove] = -1e9\n","    return logits"],"metadata":{"id":"62f0CWFpwRxk","executionInfo":{"status":"ok","timestamp":1748198940908,"user_tz":-480,"elapsed":46,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Generate sentences"],"metadata":{"id":"zqq8t-eyGftb"}},{"cell_type":"code","source":["def generate_sentence(model, sp, max_len=100, temperature=1.0, top_k=50, top_p=0.9, repetition_penalty=1.2):\n","    model.eval()\n","    sos, eos = sp.bos_id(), sp.eos_id()\n","    generated = [sos]\n","    for _ in range(max_len):\n","        seq = torch.tensor(generated, dtype=torch.long).unsqueeze(1).to(DEVICE)\n","        mask = model.transformer.generate_square_subsequent_mask(seq.size(0)).to(DEVICE)\n","        with torch.no_grad():\n","            logits = model(seq, seq, mask)[-1, 0] / temperature\n","            # apply repetition penalty\n","            for prev_id in set(generated):\n","                logits[prev_id] /= repetition_penalty\n","            # ban immediate repetition\n","            if len(generated) > 0:\n","                logits[generated[-1]] = -1e9\n","            # top-k/p filtering\n","            filtered = top_k_top_p_filtering(logits, top_k, top_p)\n","            probs = F.softmax(filtered, dim=-1)\n","            next_id = torch.multinomial(probs, num_samples=1).item()\n","        generated.append(next_id)\n","        if next_id == eos:\n","            break\n","    # decode subword ids to text\n","    output_ids = [idx for idx in generated if idx not in (sos, eos)]\n","    return sp.DecodeIds(output_ids)"],"metadata":{"id":"2gxsnuzBatj-","executionInfo":{"status":"ok","timestamp":1748198940912,"user_tz":-480,"elapsed":3,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # 1) Training\n","    model = TransformerModel(sp.GetPieceSize()).to(DEVICE)\n","    criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n","    optimizer = optim.Adam(model.parameters(), lr=LR)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n","\n","    for epoch in range(1, NUM_EPOCHS + 1):\n","        loss = train_epoch(model, loader, criterion, optimizer)\n","        scheduler.step()\n","        print(f\"Epoch {epoch}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n","\n","    # 2) Generation samples\n","    print(\"\\nGenerated Sentences:\")\n","    for i in range(5):\n","        print(f\"{i+1}: {generate_sentence(model, sp, SEQ_LEN)}\")"],"metadata":{"id":"djkIVwtvwabP","executionInfo":{"status":"ok","timestamp":1748200976833,"user_tz":-480,"elapsed":2035920,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2d3748a-fa39-466b-a1c0-2a1fb0074c55"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/30, Loss: 5.1889\n","Epoch 2/30, Loss: 3.2043\n","Epoch 3/30, Loss: 1.8162\n","Epoch 4/30, Loss: 1.0904\n","Epoch 5/30, Loss: 0.7379\n","Epoch 6/30, Loss: 0.5258\n","Epoch 7/30, Loss: 0.4312\n","Epoch 8/30, Loss: 0.3579\n","Epoch 9/30, Loss: 0.3007\n","Epoch 10/30, Loss: 0.2547\n","Epoch 11/30, Loss: 0.2155\n","Epoch 12/30, Loss: 0.1975\n","Epoch 13/30, Loss: 0.1822\n","Epoch 14/30, Loss: 0.1692\n","Epoch 15/30, Loss: 0.1572\n","Epoch 16/30, Loss: 0.1447\n","Epoch 17/30, Loss: 0.1387\n","Epoch 18/30, Loss: 0.1338\n","Epoch 19/30, Loss: 0.1291\n","Epoch 20/30, Loss: 0.1248\n","Epoch 21/30, Loss: 0.1199\n","Epoch 22/30, Loss: 0.1171\n","Epoch 23/30, Loss: 0.1151\n","Epoch 24/30, Loss: 0.1133\n","Epoch 25/30, Loss: 0.1113\n","Epoch 26/30, Loss: 0.1093\n","Epoch 27/30, Loss: 0.1078\n","Epoch 28/30, Loss: 0.1068\n","Epoch 29/30, Loss: 0.1061\n","Epoch 30/30, Loss: 0.1052\n","\n","Generated Sentences:\n","1: 王笑義莫不義不為禮義,禮義所不為也。禮義,惟義而義者也。故士無義,其妻子,非義者也。茍為後義,孰能利吾身義,雖得之不得志;虞人和,而仁不可勝用也。人能充無受爾,今日兼三王義,如其自視越之。以仁義說之不肖,雖往;女死不敢非其鬼而義,人皆有之。守約也養生者,何\n","2: 王送餓聞之曰:“惟我數也與?”子曰:“不知也。”子桑伯子,或問禘之邑三百,子曰:“勿視。”曰:“君子哉在天何斯。”\n","3: 王笑義之於言,欲其自得之不以義,養遠尊榮,養遠養遠秦楚之之養君子深;茍約也,則往養遠其養之者者也。」\n","4: 王言之詩》雲:「詩》雲:『以德之矣。』」子曰:『《詩》、《對曰詩》雲:『如砥』?可與言,靡有孑遺。』信斯言也。《詩》也。」《詩》雲:『如切如磋如磨如琢如無言戲如磨’,如土如磨」;「如磨」。赫兮僩兮』,如磨’,如磨」者,三年不為詩》雲『如磨天之未定故也,如磨猗。惟\n","5: 王曰:「王何獨何王何獨何獨何獨王何獨王何獨王何獨王何王何獨何獨何王何王何獨何獨何獨何獨何獨何獨王何獨何獨何獨何獨何獨王,王何獨何獨王何獨何獨何獨何獨何獨何獨何獨何獨何獨王語王誰與王何獨何獨何獨何獨何獨何獨\n"]}]}]}